{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bzZbMU7rqnOf"
      },
      "id": "bzZbMU7rqnOf"
    },
    {
      "cell_type": "markdown",
      "id": "87a14689",
      "metadata": {
        "id": "87a14689"
      },
      "source": [
        "# LSTM Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6629ccc3-8c3a-4121-a951-1df519278113",
      "metadata": {
        "id": "6629ccc3-8c3a-4121-a951-1df519278113"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout, BatchNormalization, Input, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46d080eb-28bb-4fb2-a36a-ac06b49fcb5b",
      "metadata": {
        "id": "46d080eb-28bb-4fb2-a36a-ac06b49fcb5b"
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "data = pd.read_csv(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43c7c458-518f-49e3-9d21-e1541fafcbd5",
      "metadata": {
        "id": "43c7c458-518f-49e3-9d21-e1541fafcbd5"
      },
      "outputs": [],
      "source": [
        "# Standardize the target column (total_amount)\n",
        "\n",
        "data[['norm_amount']] = StandardScaler().fit_transform(data[['total_amount']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3cc2fa4-9c9f-45e8-acb1-e655cab35d5e",
      "metadata": {
        "id": "f3cc2fa4-9c9f-45e8-acb1-e655cab35d5e"
      },
      "outputs": [],
      "source": [
        "# Define target column\n",
        "target_col = 'total_amount'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8a99cbd-69f8-4aed-96f8-3dee5f61c6b9",
      "metadata": {
        "id": "d8a99cbd-69f8-4aed-96f8-3dee5f61c6b9"
      },
      "outputs": [],
      "source": [
        "# Prepare features (X) and target (y)\n",
        "# Include lag features and seasonal features for the model\n",
        "X = data[[f'{target_col}_lag_{i}' for i in range(1, 11)] +  # Example using 10 lags\n",
        "          []]\n",
        "\n",
        "y = data[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5aa8ad7-3949-4aa9-a1d2-2cc45c9dfac1",
      "metadata": {
        "id": "b5aa8ad7-3949-4aa9-a1d2-2cc45c9dfac1"
      },
      "outputs": [],
      "source": [
        "# Normalize the data\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1f2747e-c040-4795-a45a-328f67697e28",
      "metadata": {
        "id": "f1f2747e-c040-4795-a45a-328f67697e28"
      },
      "outputs": [],
      "source": [
        "# Reshape data for LSTM (samples, time_steps, features)\n",
        "X = X.reshape((X.shape[0], X.shape[1], 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c66632f-2a62-4cef-a5cd-273f0c89f2a1",
      "metadata": {
        "id": "4c66632f-2a62-4cef-a5cd-273f0c89f2a1"
      },
      "outputs": [],
      "source": [
        "# Split the data into train and test sets\n",
        "train_size = int(len(X) * 0.8)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7eef3509-febd-41c2-8c2e-4812424e16b8",
      "metadata": {
        "id": "7eef3509-febd-41c2-8c2e-4812424e16b8"
      },
      "source": [
        "Long Short-Term Memory (LSTM) layers are essential for time series data analysis due to their specific design to capture temporal dependencies. These layers possess the capability to learn patterns over time, retaining pertinent information from preceding time steps while discarding irrelevant data. This attribute is paramount in time series forecasting, where past observations significantly influence future outcomes. By stacking multiple LSTM layers, the model can simultaneously learn both low-level and high-level temporal features. The initial LSTM layer captures fundamental temporal patterns, while the subsequent layer abstracts these patterns into more intricate features. Dropout and batch normalization techniques are employed to mitigate overfitting and enhance the training process. Time series models are particularly susceptible to overfitting due to the sequential nature of the data, and dropout ensures that the model does not excessively rely on specific time steps or features. Batch normalization stabilizes the learning process, which is particularly crucial in deep learning models such as LSTMs. Subsequently, the Dense layers refine the learned features and culminate in the final prediction. In time series forecasting, it is imperative to process time-dependent features prior to making predictions. This architectural choice is justified because LSTMs are well-suited to handle time-dependent data, and the combination of additional layers and regularization techniques ensures that the model strikes a balance between power and robustness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b5e1923-6826-49b1-bfa9-bdf8e02814a0",
      "metadata": {
        "id": "4b5e1923-6826-49b1-bfa9-bdf8e02814a0"
      },
      "outputs": [],
      "source": [
        "# Build the dense model\n",
        "model = Sequential()\n",
        "\n",
        "# Input Layer\n",
        "model.add(Input(shape=(X_train.shape[1], 1)))\n",
        "\n",
        "# First LSTM Layer\n",
        "model.add(LSTM(64, activation='tanh', return_sequences=True))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Second LSTM Layer\n",
        "model.add(LSTM(128, activation='tanh', return_sequences=False))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Dense Layers\n",
        "model.add(Dense(64, activation='relu', kernel_regularizer='l2'))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(1, activation='linear'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff8c83fc-4f83-489b-8a80-c68d083b1104",
      "metadata": {
        "id": "ff8c83fc-4f83-489b-8a80-c68d083b1104"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b17962ee-f92a-4dc6-82ac-c59b1193172d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b17962ee-f92a-4dc6-82ac-c59b1193172d",
        "outputId": "6932475f-d334-4123-da14-c1f2ba8af0f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 11ms/step - loss: 1.4192 - mae: 0.6969 - val_loss: 1.2906 - val_mae: 0.8463 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 10ms/step - loss: 0.4137 - mae: 0.4219 - val_loss: 0.7346 - val_mae: 0.6471 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 11ms/step - loss: 0.2116 - mae: 0.3056 - val_loss: 0.8640 - val_mae: 0.7194 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 11ms/step - loss: 0.1597 - mae: 0.2685 - val_loss: 0.5602 - val_mae: 0.5807 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 10ms/step - loss: 0.1341 - mae: 0.2472 - val_loss: 0.2672 - val_mae: 0.3819 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 11ms/step - loss: 0.1191 - mae: 0.2317 - val_loss: 0.1011 - val_mae: 0.2260 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 11ms/step - loss: 0.1136 - mae: 0.2278 - val_loss: 0.1981 - val_mae: 0.3433 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 11ms/step - loss: 0.1027 - mae: 0.2157 - val_loss: 0.2458 - val_mae: 0.3814 - learning_rate: 0.0010\n",
            "Epoch 9/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 11ms/step - loss: 0.0955 - mae: 0.2083 - val_loss: 0.0885 - val_mae: 0.2189 - learning_rate: 0.0010\n",
            "Epoch 10/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 11ms/step - loss: 0.0882 - mae: 0.1998 - val_loss: 0.1042 - val_mae: 0.2383 - learning_rate: 0.0010\n",
            "Epoch 11/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 11ms/step - loss: 0.0845 - mae: 0.1948 - val_loss: 0.2370 - val_mae: 0.3882 - learning_rate: 0.0010\n",
            "Epoch 12/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 11ms/step - loss: 0.0818 - mae: 0.1923 - val_loss: 0.0850 - val_mae: 0.2206 - learning_rate: 0.0010\n",
            "Epoch 13/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 11ms/step - loss: 0.0800 - mae: 0.1889 - val_loss: 0.2167 - val_mae: 0.3889 - learning_rate: 0.0010\n",
            "Epoch 14/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 11ms/step - loss: 0.0794 - mae: 0.1882 - val_loss: 0.1160 - val_mae: 0.2474 - learning_rate: 0.0010\n",
            "Epoch 15/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 11ms/step - loss: 0.0764 - mae: 0.1854 - val_loss: 0.0988 - val_mae: 0.2316 - learning_rate: 0.0010\n",
            "Epoch 16/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 11ms/step - loss: 0.0723 - mae: 0.1799 - val_loss: 0.1106 - val_mae: 0.2567 - learning_rate: 0.0010\n",
            "Epoch 17/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 11ms/step - loss: 0.0741 - mae: 0.1807 - val_loss: 0.0640 - val_mae: 0.1881 - learning_rate: 0.0010\n",
            "Epoch 18/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 11ms/step - loss: 0.0732 - mae: 0.1794 - val_loss: 0.1089 - val_mae: 0.2532 - learning_rate: 0.0010\n",
            "Epoch 19/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 11ms/step - loss: 0.0708 - mae: 0.1759 - val_loss: 0.0674 - val_mae: 0.1902 - learning_rate: 0.0010\n",
            "Epoch 20/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 11ms/step - loss: 0.0685 - mae: 0.1743 - val_loss: 0.0543 - val_mae: 0.1653 - learning_rate: 0.0010\n",
            "Epoch 21/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 11ms/step - loss: 0.0666 - mae: 0.1713 - val_loss: 0.0948 - val_mae: 0.2341 - learning_rate: 0.0010\n",
            "Epoch 22/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 11ms/step - loss: 0.0660 - mae: 0.1701 - val_loss: 0.0599 - val_mae: 0.1873 - learning_rate: 0.0010\n",
            "Epoch 23/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 11ms/step - loss: 0.0666 - mae: 0.1706 - val_loss: 0.0653 - val_mae: 0.1893 - learning_rate: 0.0010\n",
            "Epoch 24/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 11ms/step - loss: 0.0653 - mae: 0.1693 - val_loss: 0.1637 - val_mae: 0.3318 - learning_rate: 0.0010\n",
            "Epoch 25/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 11ms/step - loss: 0.0629 - mae: 0.1668 - val_loss: 0.0458 - val_mae: 0.1502 - learning_rate: 0.0010\n",
            "Epoch 26/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 11ms/step - loss: 0.0654 - mae: 0.1678 - val_loss: 0.0432 - val_mae: 0.1406 - learning_rate: 0.0010\n",
            "Epoch 27/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 11ms/step - loss: 0.0604 - mae: 0.1625 - val_loss: 0.0312 - val_mae: 0.1092 - learning_rate: 0.0010\n",
            "Epoch 28/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 11ms/step - loss: 0.0620 - mae: 0.1638 - val_loss: 0.0322 - val_mae: 0.1250 - learning_rate: 0.0010\n",
            "Epoch 29/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 11ms/step - loss: 0.0593 - mae: 0.1603 - val_loss: 0.0460 - val_mae: 0.1517 - learning_rate: 0.0010\n",
            "Epoch 30/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 11ms/step - loss: 0.0600 - mae: 0.1608 - val_loss: 0.0360 - val_mae: 0.1312 - learning_rate: 0.0010\n",
            "Epoch 31/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 11ms/step - loss: 0.0604 - mae: 0.1615 - val_loss: 0.0344 - val_mae: 0.1372 - learning_rate: 0.0010\n",
            "Epoch 32/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 11ms/step - loss: 0.0601 - mae: 0.1611 - val_loss: 0.0321 - val_mae: 0.1218 - learning_rate: 0.0010\n",
            "Epoch 33/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 11ms/step - loss: 0.0512 - mae: 0.1482 - val_loss: 0.0224 - val_mae: 0.1000 - learning_rate: 5.0000e-04\n",
            "Epoch 34/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 11ms/step - loss: 0.0491 - mae: 0.1467 - val_loss: 0.0147 - val_mae: 0.0712 - learning_rate: 5.0000e-04\n",
            "Epoch 35/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 11ms/step - loss: 0.0507 - mae: 0.1476 - val_loss: 0.0158 - val_mae: 0.0798 - learning_rate: 5.0000e-04\n",
            "Epoch 36/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 11ms/step - loss: 0.0495 - mae: 0.1462 - val_loss: 0.0235 - val_mae: 0.1128 - learning_rate: 5.0000e-04\n",
            "Epoch 37/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 11ms/step - loss: 0.0492 - mae: 0.1471 - val_loss: 0.0199 - val_mae: 0.0948 - learning_rate: 5.0000e-04\n",
            "Epoch 38/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 11ms/step - loss: 0.0485 - mae: 0.1449 - val_loss: 0.0219 - val_mae: 0.0956 - learning_rate: 5.0000e-04\n",
            "Epoch 39/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 11ms/step - loss: 0.0491 - mae: 0.1462 - val_loss: 0.0209 - val_mae: 0.0898 - learning_rate: 5.0000e-04\n",
            "Epoch 40/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 11ms/step - loss: 0.0449 - mae: 0.1392 - val_loss: 0.0170 - val_mae: 0.0863 - learning_rate: 2.5000e-04\n",
            "Epoch 41/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 11ms/step - loss: 0.0447 - mae: 0.1396 - val_loss: 0.0174 - val_mae: 0.0964 - learning_rate: 2.5000e-04\n",
            "Epoch 42/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 11ms/step - loss: 0.0428 - mae: 0.1376 - val_loss: 0.0158 - val_mae: 0.0884 - learning_rate: 2.5000e-04\n",
            "Epoch 43/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 11ms/step - loss: 0.0430 - mae: 0.1373 - val_loss: 0.0165 - val_mae: 0.0831 - learning_rate: 2.5000e-04\n",
            "Epoch 44/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 11ms/step - loss: 0.0435 - mae: 0.1374 - val_loss: 0.0237 - val_mae: 0.1089 - learning_rate: 2.5000e-04\n",
            "Epoch 45/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 11ms/step - loss: 0.0404 - mae: 0.1336 - val_loss: 0.0116 - val_mae: 0.0662 - learning_rate: 1.2500e-04\n",
            "Epoch 46/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 11ms/step - loss: 0.0415 - mae: 0.1339 - val_loss: 0.0118 - val_mae: 0.0678 - learning_rate: 1.2500e-04\n",
            "Epoch 47/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 11ms/step - loss: 0.0395 - mae: 0.1311 - val_loss: 0.0124 - val_mae: 0.0719 - learning_rate: 1.2500e-04\n",
            "Epoch 48/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 11ms/step - loss: 0.0406 - mae: 0.1319 - val_loss: 0.0143 - val_mae: 0.0765 - learning_rate: 1.2500e-04\n",
            "Epoch 49/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 11ms/step - loss: 0.0405 - mae: 0.1324 - val_loss: 0.0134 - val_mae: 0.0727 - learning_rate: 1.2500e-04\n",
            "Epoch 50/50\n",
            "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 11ms/step - loss: 0.0410 - mae: 0.1333 - val_loss: 0.0173 - val_mae: 0.0825 - learning_rate: 1.2500e-04\n",
            "Restoring model weights from the end of the best epoch: 45.\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1)\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), callbacks=[lr_scheduler, early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d5eb121-edd8-4ffd-8f90-90bce07d64ad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d5eb121-edd8-4ffd-8f90-90bce07d64ad",
        "outputId": "57e4dc55-e780-41ba-b3ac-3edff84caa8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "y_pred = model.predict(X_test).flatten()\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a99748e-0435-4383-9a9e-2a56a4391cd3",
      "metadata": {
        "id": "7a99748e-0435-4383-9a9e-2a56a4391cd3"
      },
      "source": [
        "Prints the evaluation metrics for the model’s performance in a formatted manner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a63d902-9526-4d9a-8c0c-35eee2f63b20",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a63d902-9526-4d9a-8c0c-35eee2f63b20",
        "outputId": "0227143b-b6ab-4848-9308-1ae009543cb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Evaluation Metrics:\n",
            "Mean Absolute Error (MAE): 0.07\n",
            "Mean Squared Error (MSE): 0.01\n",
            "Root Mean Squared Error (RMSE): 0.10\n",
            "R-squared (R²): 0.99\n"
          ]
        }
      ],
      "source": [
        "print(\"Model Evaluation Metrics:\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
        "print(f\"R-squared (R²): {r2:.2f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}